# Description of algorithm(s) implemented
Q: Describe your approach and explain why you chose it over alternative approaches

My Approach:
After a lot of experimentation I decided on using the TD(0) algorithm as it was giving the best results among the algorithms I tried.

Explanation and why I chose it over other options:

Why reject Model Based:
From the start I rejected using any model based approach where we would learn T^ and R^ since we have data available for the transistions (S0, a0, R0, S1) corresponding to only a single policy pi(S), so we cannot learn T^ and R^ of the underlying mdp from that data and later evaluate the policy using Bellman's equations.

Monte Carlo:
Further, using monte-carlo methods would give incorrect results since the underlying task is not episodic and the file given to us has been terminated at an arbitary point of time. They don't work (in non episodic case) since Monte Carlo methods only make updates at the end of an episode. If gamma (discount factor) is small, using a method like every visit Monte carlo (I tried this) can give approximately correct results but it can fail when there are a larger number of states or low number of samples and a high (close to 1) gamma value. 
*In fact every visit mc algorithm (Arbitarily stopping at the end) was giving slightly better results on the given files "d1.txt" and "d2.txt" but I decided not to use it due to theoretical incorrectness. Further it performs poorly (I tried) when the fewer data points are available*

TD Temporal difference method: TD(0), TD(lambda) and TD(0) with 2 step returns
Ruling out the above options leaves the choice between temporal difference methods
I thought that TD(lambda) would perform well since it combines the sampling of monte-carlo with the boot-strapping of Temporal difference (TD) methods. But in practice it was not performing better than TD(0) on the given test cases (I wrote code and tried) and since the instructor had discussed some targets for these 2 files in class, I chose TD(0) over TD(lambda) method.
I also tried using TD(0) with:
2 step returns --> V(s_{t+1}) = V(s_{t}) + alpha*(r_t + gamma*r_{t+1} + gamma^2*V(s^{t+2}) - V(s_t))
and this method was performing better for some specfic instances but not uniformly. Also the learning was slower for this method as compared to TD(0)
So finally chose TD(0) since it was giving uniformly better performance


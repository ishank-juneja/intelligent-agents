# Code observations
For large values of gamma i.e 1 and closer to 1 the solutions between LP optimization and Policy iteration do not match to 6 decimal places, closer to 4 decimal places. This is especially true for a large number of states and or actions
I think this has to do with numerical instability during matrix inversion in case of Policy Iteration approach

# MDP family observations
The general approach I used was to think of a fixed start state and end state. We can start from any state but that would just represent being born into a later state of the same start to end journey.
After this there were 3 possible paths to get to the end with each being locally optimal over the required intervals of discount factor gamma.   